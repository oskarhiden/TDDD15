---
title: "TDDE15 - Lab 1"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Question 1
```{r}
library(bnlearn)
data("asia")
structure = hc(asia, restart = 3) #start = initial structure, restart = random restarts, score = score, equvivilent sample size.

b=T
for (i in 1:10) {
  structure2 = hc(asia, restart = 1)
  b = all.equal(structure, structure2)
  if (b!=TRUE) {
    break
  } 
}
plot(structure)
plot(structure2)

```
The HC return two different networks, because it can get trapped in a local optimum. HC gives you all independencies that exist in the true model. it does not give you any fals indenpendancies. In this case an edge is reversed, which would give the same score in the HC algorithm.

## Question 2
```{r}
N = dim(asia)[1]
N
train = asia[1:floor(N*0.8),]
test = asia[(floor(N*0.8)+1):N,]

structure = hc(train, restart = 0)
plot(structure)

#Learn conditional probabiliies given the nodes parents
fit = bn.fit(structure, data=train)
fit
coefficients(fit)

# Create Graphical independance network ( grain object )
fit_grain = as.grain(fit)
fit_grain

# create a junction tree and est. potential clique ( grain object )
junc_tree = compile(fit_grain)
junc_tree

#remove S from test-data
test_ans = test[,"S"]
test_evid = subset(test, select = -2)

#Predict S
pred_s =c()
for (j in 1:dim(test_evid[1])) {
  
# finding/evidance or potentials
#need to extract observed values correctly.....
obs = c()
for (i in 1:7) {
  obs = c(obs, as.character(test_evid[j,i]))
}

nodes_ev = names(test_evid)
evid = setEvidence(fit_grain, nodes_ev, states = obs)
pEvidence(evid)

# quergrain to get conditional distributon
node = c("S")
prob_s = querygrain(evid, nodes = node)

if (prob_s$S[1]>prob_s$S[2]) {
  pred_s=c(pred_s,"no")
}else{
  pred_s=c(pred_s,"yes")
}
}

table(pred_s,test_ans)


```